training_data_paths:
- "/content/chronos-forecasting/Synthetic_Data/pretrian/tsmixup_data.arrow"
- "/content/chronos-forecasting/Synthetic_Data/pretrian/kernelsynth_data.arrow"
probability:
- 0.9
- 0.1
# Model configuration - T5-tiny for T4 GPU
model_id: "google/t5-efficient-tiny"
model_type: "seq2seq"

# Training parameters - reduced for T4 GPU
max_steps: 50000
save_steps: 5000
log_steps: 100
per_device_train_batch_size: 8  # Reduced from 16 for T4
learning_rate: 0.001
gradient_accumulation_steps: 4  # Increased to maintain effective batch size of 32

# Data processing parameters - adjusted for memory
context_length: 256  # Reduced from 512 for T4
prediction_length: 64
min_past: 64
shuffle_buffer_length: 100
dataloader_num_workers: 1  # Reduced for Colab
max_missing_prop: 0.9

# Rest of the config remains same
tokenizer_class: "MeanScaleUniformBins"
tokenizer_kwargs: {'low_limit': -15.0, 'high_limit': 15.0}
n_tokens: 4096
n_special_tokens: 2
pad_token_id: 0
eos_token_id: 1
use_eos_token: true

num_samples: 20
temperature: 1.0
top_k: 50
top_p: 1.0

tf32: false
torch_compile: true

output_dir: "./chronos_output/"
